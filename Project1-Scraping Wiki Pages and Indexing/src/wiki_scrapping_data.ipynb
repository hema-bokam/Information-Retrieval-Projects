{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZ1I5drFNtiwDj3J4S0qP0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVpph9g19Yxp","executionInfo":{"status":"ok","timestamp":1727456173784,"user_tz":240,"elapsed":1,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"outputId":"bf987b5b-6918-4728-c88b-de9c0a71e021"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n"]}],"source":["!pip install wikipedia\n","\n","import wikipedia\n","import re\n","import random\n","import time\n"]},{"cell_type":"code","source":["# make sure text has only alphanumeric data\n","def remove_extra_characters(text):\n","    text = re.sub(r'[^a-zA-Z0-9 ]+', '', text)\n","    return text"],"metadata":{"id":"WwrMmsT8IGFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find count of keywords match\n","def relevance_score(text, keywords):\n","    score = sum(text.lower().count(keyword.lower()) for keyword in keywords)\n","    return score"],"metadata":{"id":"VbCQPAxfMmMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scrape a single Wikipedia page\n","def scrape_wikipedia_pages(title, topic, keywords):\n","    try:\n","        content = wikipedia.page(title, auto_suggest=False)\n","        summary = remove_extra_characters(content.summary)\n","        revision_id = content.revision_id\n","        title = content.title\n","        url = content.url\n","\n","        document = {\n","            \"revision_id\": revision_id,\n","            \"title\": title,\n","            \"url\": url,\n","            \"summary\": summary,\n","            \"topic\": topic\n","        }\n","\n","        score = relevance_score(summary, keywords) + relevance_score(title, keywords)\n","        document[\"score\"] = score\n","\n","        if len(summary) > 200 and score >= 1:\n","            return document, content.links\n","        else:\n","            return None, []\n","\n","    except wikipedia.exceptions.DisambiguationError:\n","        print(f\"disambiguation error for: {title}\")\n","        return None, []\n","    except wikipedia.exceptions.PageError:\n","        print(f\"page is not found: {title}\")\n","        return None, []\n","    except Exception as e:\n","        print(f\"some unexpected error occurred: {e}\")\n","        return None, []"],"metadata":{"id":"Ug8qoz1FWJHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scrape wikipedia data\n","def scrape_wikipedia_subtopics(main_topic, sub_topics, keywords, min_docs_per_topic):\n","    results = []\n","    extra_links = []\n","    unique_urls = set()\n","    print(f\"Scraping summaries for main topic: {main_topic}\")\n","\n","    for sub_topic in sub_topics:\n","        print(f\"  Scraping sub-topic: {sub_topic}\")\n","        search_results = []\n","        attempts = 0\n","        max_attempts = 3\n","        while attempts < max_attempts:\n","            try:\n","                search_results = wikipedia.search(sub_topic, results=300)\n","                print(len(search_results))\n","                break\n","            except wikipedia.exceptions.HTTPTimeoutError:\n","                print(\"HTTP timeout error occurred. attempt num: \", attempts)\n","                attempts += 1\n","                wait_time = 2 ** attempts\n","                time.sleep(wait_time)\n","            except wikipedia.exceptions.WikipediaException as e:\n","                print(f\"error occured during search: {e}\")\n","                break\n","\n","        for page_title in search_results:\n","            # print(f\"current scraping page: {page_title}\")\n","            document, links = scrape_wikipedia_pages(page_title, main_topic, keywords)\n","            if document and document['url'] not in unique_urls:\n","                results.append(document)\n","                unique_urls.add(document['url'])\n","                if links:\n","                  extra_links.extend(links)\n","\n","            if len(results) >= min_docs_per_topic:\n","                break\n","        print(f\"total documents scraped till topic: {len(results)}, {sub_topic}\")\n","        if len(results) >= min_docs_per_topic:\n","            break\n","    # Scrape additional pages if needed\n","    print(\"total documents scrapped: \", len(results))\n","    print(\"total extra links: \", len(extra_links))\n","    while len(results) < min_docs_per_topic and len(extra_links) > 0:\n","        page_title = extra_links.pop()\n","        try:\n","            document, links = scrape_wikipedia_pages(page_title, main_topic, keywords)\n","            if document and document['url'] not in unique_urls:\n","                unique_urls.add(document['url'])\n","                results.append(document)\n","                # extra_links.extend(links)\n","\n","            if len(results) >= min_docs_per_topic:  # Stop if we have enough documents\n","                break\n","\n","        except wikipedia.exceptions.DisambiguationError:\n","            continue\n","        except Exception as e:\n","            continue\n","\n","    # print(\"final total documents scrapped: \", len(results))\n","    results.sort(key=lambda x: x.get('score', 0), reverse=True)  # Sort directly on results\n","    # remove score term in document\n","    for doc in results:\n","        doc.pop('score', None)\n","    # send top 510 results\n","    return results"],"metadata":{"id":"E7G27rbQPQR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topics_list = {\n","    \"Health\": [\"Common diseases\", \"Infectious diseases\", \"Mental health trends\", \"Health care system\", \"Global Health statistics\"],\n","    \"Environment\": [\"Global warming\", \"Climate Change\", \"Waste Management\", \"Greenhouse gases\", \"Deforestation rates\", \"Endangered species\"],\n","    \"Technology\": [\"Emerging technologies\", \"AI advancements\", \"Software Development\", \"Could Computing issues\", \"Computer programming\"],\n","    \"Economy\": [\"Stock market performance\", \"Job markets\", \"Cryptocurrency trends\", \"Bitcoin trends\", \"Trading strategies\", \"Currency Exchange rates\"],\n","    \"Entertainment\": [\"Music industry\", \"Popular cultural events\", \"Streaming platforms\", \"Film industry\", \"Digital media\"],\n","    \"Sports\": [\"Major sporting events\", \"Sports analytics\", \"Olympic Games\", \"Cricket test matches\", \"football tournaments\"],\n","    \"Politics\": [\"Elections\", \"Public policy analysis\", \"International relations\", \"Political parties\"],\n","    \"Education\": [\"Literacy rates\", \"Online education trends\", \"Student loan data\", \"higher education\", \"science education\", \"Education policy\" ],\n","    \"Travel\": [\"Top tourist destinations\", \"Airline industry data\", \"Travel trends\", \"International tourism\", \"World Tour\", \"Adventure travel\", \"Beach destinations\", \"Travel deals\", \"Business trip\"],\n","    \"Food\": [\"Organic Farming\",\"Crop yield statistics\", \"Global hunger\", \"Food security\", \"food quality\", \"Protein food\", \"healthy drinks\", \"healthy food\", \"Fruits\", \"malnutrition\", \"dairy products\"]\n","}\n","\n","\n","# keywords for each main topic\n","keywords = {\n","    \"Health\": [\"common health diseases\", \"common disease\", \"common diseases\", \"diseases\", \"Infectious diseases\", \"health statistics\", \"Global health statistics\", \"health statistics\", \"global health\",\"healthcare system\", \"Health care system\", \"health system\", \"mental health\", \"mental health trends\"],\n","    \"Environment\": [\"global warming\", \"climate change\", \"waste management\", \"endangered species\",\"deforestation\", \"deforestation rates\", \"greenhouse gases\", \"co2\", \"manage waste\"],\n","    \"Technology\": [\"artificial intelligence\", \"machine learning\", \"blockchain\", \"AI advancements\", \"Artificial Intelligence Advancements\", \"Emerging technologies\",\"quantum computing\", \"AI trends\", \"Artificial Intelligence trends\", \"Web development\", \"Software development\", \"API\", \"Could computing\", \"Software engineering\", \"Computer\", \"programming\"],\n","    \"Economy\": [\"stock market performance\",\"stocks\", \"job\",\"recession\", \"job loss\", \"trading\", \"trading strategy\",\"trading strategies\",\"currency\", \"currency exchange\", \"currency exchange rate\" \"Job markets\", \"Job market\", \"cryptocurrency trends\", \"cryptocurrency\", \"Bitcoin\", \"Bitcoin trends\", \"stock market\"],\n","    \"Entertainment\": [\"Music industry\", \"Popular events\",\"cultural events\", \"cultural activities\", \"Music\", \"online streaming platforms\", \"Streaming platforms\", \"Film industry\", \"Film industries\", \"Music industries\", \"digital media\"],\n","    \"Sports\": [\"sports events\", \"Major sporting events\", \"sporting events\", \"Sports analytics\", \"Olympic games\", \"Olympic sports\", \"games\", \"Olympics\", \"test matches\", \"cricket\", \"cricket matches\", \"football tournaments\", \"football\"],\n","    \"Politics\": [\"Elections\", \"Public policy analysis\", \"International relations\", \"Political parties\", \"Public policy\"],\n","    \"Education\": [ \"Literacy rates\", \"Education policy\",\"Online education trend\", \"Online education trends\", \"Student loan data\", \"Students loans data\", \"higher education\", \"student loan\", \"Online learning trends\", \"science\", \"science education\"],\n","    \"Travel\": [\"Top tourist destinations\", \"Airline industry data\", \"Travel trends\", \"International tourism\",\"International travel\", \"Airline data\", \"tourist destinations\", \"best tourist places\", \"top tourist places\", \"World tour\", \"Adventure travel\", \"Beach destinations\",\"Beaches\", \"travel deals\", \"Business trip\"],\n","    \"Food\": [\"Organic Farming\",\"Crop yield statistics\", \"Global hunger\", \"Food security\", \"food quality\", \"Protein rich food\", \"protein food\", \"high protein food\", \"healthy drinks\", \"healthy food\", \"Fruits\", \"malnutrition\",  \"dairy products\"]\n","}\n"],"metadata":{"id":"JblU-fv7XYmU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_documents = {}"],"metadata":{"id":"vuK5rycQx3eW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scrap Health data\n","health_results = scrape_wikipedia_subtopics(\"Health\", topics_list[\"Health\"], keywords[\"Health\"], 520)\n","total_documents[\"Health\"] = health_results\n","print(len(health_results))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7b_ZFd5MxcAd","executionInfo":{"status":"ok","timestamp":1727481507697,"user_tz":240,"elapsed":788178,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"outputId":"a48b39f2-fc8d-4e40-f8f6-f355ac974f03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping summaries for main topic: Health\n","  Scraping sub-topic: Common diseases\n","300\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  lis = BeautifulSoup(html).find_all('li')\n"]},{"output_type":"stream","name":"stdout","text":["disambiguation error for: Paget's disease\n","total documents scraped till topic: 144, Common diseases\n","  Scraping sub-topic: Infectious diseases\n","300\n","disambiguation error for: National Institute of Infectious Diseases\n","total documents scraped till topic: 270, Infectious diseases\n","  Scraping sub-topic: Mental health trends\n","300\n","total documents scraped till topic: 413, Mental health trends\n","  Scraping sub-topic: Health care system\n","300\n","disambiguation error for: Health care (disambiguation)\n","total documents scraped till topic: 520, Health care system\n","total documents scrapped:  520\n","total extra links:  147333\n","520\n"]}]},{"cell_type":"code","source":["environment_results = scrape_wikipedia_subtopics(\"Environment\", topics_list[\"Environment\"], keywords[\"Environment\"], 650)\n","total_documents[\"Environment\"] = environment_results\n","print(len(environment_results))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYSARTs3yA8n","executionInfo":{"status":"ok","timestamp":1727457194383,"user_tz":240,"elapsed":996072,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"outputId":"201155f1-6536-4e68-8397-0847681b7ff0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping summaries for main topic: Environment\n","  Scraping sub-topic: Global warming\n","300\n","disambiguation error for: Global warming (disambiguation)\n","disambiguation error for: Unstoppable global warming\n","disambiguation error for: Climate change and agriculture\n","disambiguation error for: Climate change (disambiguation)\n","total documents scraped till topic: 209, Global warming\n","  Scraping sub-topic: Climate Change\n","300\n","disambiguation error for: Climate change (disambiguation)\n","disambiguation error for: Climate change and agriculture\n","disambiguation error for: Climate Change Bill\n","disambiguation error for: Climate change in Georgia\n","disambiguation error for: Climate policy\n","total documents scraped till topic: 393, Climate Change\n","  Scraping sub-topic: Waste Management\n","300\n","disambiguation error for: Waste management (disambiguation)\n","disambiguation error for: Radioactive waste disposal\n","disambiguation error for: Dumping\n","total documents scraped till topic: 517, Waste Management\n","  Scraping sub-topic: Greenhouse gases\n","300\n","disambiguation error for: Climate change (disambiguation)\n","disambiguation error for: GWG\n","total documents scraped till topic: 649, Greenhouse gases\n","  Scraping sub-topic: Deforestation rates\n","300\n","total documents scraped till topic: 650, Deforestation rates\n","total documents scrapped:  650\n","total extra links:  229848\n","650\n"]}]},{"cell_type":"code","source":["technology_results = scrape_wikipedia_subtopics(\"Technology\", topics_list[\"Technology\"], keywords[\"Technology\"], 520)\n","total_documents[\"Technology\"] = technology_results\n","print(len(technology_results))"],"metadata":{"id":"V4bWqAI2yKZN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727480292288,"user_tz":240,"elapsed":644286,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"outputId":"38ae94cf-3eeb-4020-d742-8e609c7543e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping summaries for main topic: Technology\n","  Scraping sub-topic: Emerging technologies\n","300\n","disambiguation error for: ICCT\n","disambiguation error for: The Net\n","total documents scraped till topic: 185, Emerging technologies\n","  Scraping sub-topic: AI advancements\n","300\n","total documents scraped till topic: 354, AI advancements\n","  Scraping sub-topic: Software Development\n","300\n","disambiguation error for: Development\n","total documents scraped till topic: 520, Software Development\n","total documents scrapped:  520\n","total extra links:  114395\n","520\n"]}]},{"cell_type":"code","source":["economy_results = scrape_wikipedia_subtopics(\"Economy\", topics_list[\"Economy\"], keywords[\"Economy\"], 650)\n","total_documents[\"Economy\"] = economy_results\n","print(len(economy_results))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8NdHOA7yNqY","executionInfo":{"status":"ok","timestamp":1727458374784,"user_tz":240,"elapsed":1162972,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"outputId":"871626d0-5565-4617-baf1-06cdaaa16bf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping summaries for main topic: Economy\n","  Scraping sub-topic: Stock market performance\n","300\n","disambiguation error for: Hang Seng (disambiguation)\n","total documents scraped till topic: 179, Stock market performance\n","  Scraping sub-topic: Job markets\n","300\n","disambiguation error for: Job loss\n","disambiguation error for: EJM\n","disambiguation error for: Bax\n","total documents scraped till topic: 322, Job markets\n","  Scraping sub-topic: Cryptocurrency trends\n","300\n","total documents scraped till topic: 407, Cryptocurrency trends\n","  Scraping sub-topic: Bitcoin trends\n","300\n","total documents scraped till topic: 436, Bitcoin trends\n","  Scraping sub-topic: Trading strategies\n","300\n","disambiguation error for: Spread\n","disambiguation error for: Jelly roll\n","disambiguation error for: Strategy (disambiguation)\n","disambiguation error for: Basis\n","disambiguation error for: Gut\n","disambiguation error for: Reversal\n","disambiguation error for: Strip\n","disambiguation error for: Roll\n","total documents scraped till topic: 590, Trading strategies\n","  Scraping sub-topic: Currency Exchange rates\n","300\n","total documents scraped till topic: 650, Currency Exchange rates\n","total documents scrapped:  650\n","total extra links:  161210\n","650\n"]}]},{"cell_type":"code","source":["entertainment_results = scrape_wikipedia_subtopics(\"Entertainment\", topics_list[\"Entertainment\"], keywords[\"Entertainment\"], 650)\n","total_documents[\"Entertainment\"] = entertainment_results\n","print(len(entertainment_results))"],"metadata":{"id":"avg0vj9gyQaw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6490a6ff-9046-4af5-cda9-60e94e12bbfe","executionInfo":{"status":"ok","timestamp":1727460582722,"user_tz":240,"elapsed":954139,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping summaries for main topic: Entertainment\n","  Scraping sub-topic: Music industry\n","300\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  lis = BeautifulSoup(html).find_all('li')\n"]},{"output_type":"stream","name":"stdout","text":["disambiguation error for: Music Industry Arts\n","disambiguation error for: Industry\n","total documents scraped till topic: 267, Music industry\n","  Scraping sub-topic: Popular cultural events\n","300\n","total documents scraped till topic: 337, Popular cultural events\n","  Scraping sub-topic: Streaming platforms\n","300\n","disambiguation error for: Stream (disambiguation)\n","total documents scraped till topic: 507, Streaming platforms\n","  Scraping sub-topic: Film industry\n","300\n","disambiguation error for: Tollywood\n","disambiguation error for: Bengali film\n","disambiguation error for: Industry\n","total documents scraped till topic: 650, Film industry\n","total documents scrapped:  650\n","total extra links:  271292\n","650\n"]}]},{"cell_type":"code","source":["sports_results = scrape_wikipedia_subtopics(\"Sports\", topics_list[\"Sports\"], keywords[\"Sports\"], 650)\n","total_documents[\"Sports\"] = sports_results\n","print(len(sports_results))"],"metadata":{"id":"BlmOh9fKyRqq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727462072018,"user_tz":240,"elapsed":967913,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"outputId":"e1dfd3f2-4691-4bcc-93c5-9fb5e60f2841"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping summaries for main topic: Sports\n","  Scraping sub-topic: Major sporting events\n","300\n","disambiguation error for: Postgame\n","disambiguation error for: Overlay\n","disambiguation error for: Pregame\n","total documents scraped till topic: 157, Major sporting events\n","  Scraping sub-topic: Sports analytics\n","300\n","disambiguation error for: SSAC\n","disambiguation error for: GA\n","disambiguation error for: Uba\n","total documents scraped till topic: 280, Sports analytics\n","  Scraping sub-topic: Olympic Games\n","300\n","disambiguation error for: Paris Olympics\n","disambiguation error for: London Olympics\n","disambiguation error for: Olympic\n","disambiguation error for: Greek Olympics\n","total documents scraped till topic: 542, Olympic Games\n","  Scraping sub-topic: Cricket test matches\n","300\n","disambiguation error for: Test match\n","total documents scraped till topic: 650, Cricket test matches\n","total documents scrapped:  650\n","total extra links:  298865\n","650\n"]}]},{"cell_type":"code","source":["politics_results = scrape_wikipedia_subtopics(\"Politics\", topics_list[\"Politics\"], keywords[\"Politics\"], 520)\n","total_documents[\"Politics\"] = politics_results\n","print(len(politics_results))"],"metadata":{"id":"2OL8sPm5yVTw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727478069680,"user_tz":240,"elapsed":656498,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"outputId":"2f6f4c1f-0c4e-4ca5-e028-de8b24d6a108"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping summaries for main topic: Politics\n","  Scraping sub-topic: Elections\n","300\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  lis = BeautifulSoup(html).find_all('li')\n"]},{"output_type":"stream","name":"stdout","text":["disambiguation error for: 2005 Iraqi elections\n","disambiguation error for: Elections in Ireland\n","total documents scraped till topic: 273, Elections\n","  Scraping sub-topic: Public policy analysis\n","300\n","disambiguation error for: Institute for Policy Research\n","total documents scraped till topic: 418, Public policy analysis\n","  Scraping sub-topic: International relations\n","300\n","disambiguation error for: Committee on International Relations\n","disambiguation error for: Institute of International Relations\n","disambiguation error for: International affairs (disambiguation)\n","disambiguation error for: Realism\n","total documents scraped till topic: 520, International relations\n","total documents scrapped:  520\n","total extra links:  285343\n","520\n"]}]},{"cell_type":"code","source":["education_results = scrape_wikipedia_subtopics(\"Education\", topics_list[\"Education\"], keywords[\"Education\"], 520)\n","total_documents[\"Education\"] = education_results\n","print(len(education_results))"],"metadata":{"id":"52MIXCOiyXeb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727468649608,"user_tz":240,"elapsed":1000405,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"outputId":"a40bdff7-fa7e-4f40-bda1-dfb2ff6525d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping summaries for main topic: Education\n","  Scraping sub-topic: Literacy rates\n","300\n","total documents scraped till topic: 44, Literacy rates\n","  Scraping sub-topic: Online education trends\n","300\n","total documents scraped till topic: 127, Online education trends\n","  Scraping sub-topic: Student loan data\n","300\n","total documents scraped till topic: 218, Student loan data\n","  Scraping sub-topic: higher education\n","300\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  lis = BeautifulSoup(html).find_all('li')\n"]},{"output_type":"stream","name":"stdout","text":["disambiguation error for: Higher Education Commission\n","disambiguation error for: Higher Education (disambiguation)\n","disambiguation error for: Minister for Higher Education\n","disambiguation error for: Higher Education Act\n","disambiguation error for: National Council for Higher Education\n","total documents scraped till topic: 409, higher education\n","  Scraping sub-topic: science education\n","300\n","disambiguation error for: Centre for Science Education\n","disambiguation error for: Ministry of Education, Science and Culture\n","disambiguation error for: Ministry of Education, Science and Technology\n","disambiguation error for: Education and Science Workers' Union\n","total documents scraped till topic: 520, science education\n","total documents scrapped:  520\n","total extra links:  159274\n","520\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"PlOnGyLXFbfc"}},{"cell_type":"code","source":["travel_results = scrape_wikipedia_subtopics(\"Travel\", topics_list[\"Travel\"], keywords[\"Travel\"], 520)\n","total_documents[\"Travel\"] = travel_results\n","print(len(travel_results))"],"metadata":{"id":"SLe3_H-jycNC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727472075899,"user_tz":240,"elapsed":1644320,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"outputId":"95b9ec0c-a846-4ef3-8554-d24f27ecf768"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping summaries for main topic: Travel\n","  Scraping sub-topic: Top tourist destinations\n","300\n","total documents scraped till topic: 85, Top tourist destinations\n","  Scraping sub-topic: Airline industry data\n","300\n","disambiguation error for: ADR\n","disambiguation error for: Hermes (disambiguation)\n","disambiguation error for: PT\n","total documents scraped till topic: 89, Airline industry data\n","  Scraping sub-topic: Travel trends\n","300\n","total documents scraped till topic: 104, Travel trends\n","  Scraping sub-topic: International tourism\n","300\n","total documents scraped till topic: 168, International tourism\n","  Scraping sub-topic: World Tour\n","300\n","disambiguation error for: World Tour\n","total documents scraped till topic: 355, World Tour\n","  Scraping sub-topic: Adventure travel\n","300\n","disambiguation error for: Tim Cahill (disambiguation)\n","disambiguation error for: KE\n","disambiguation error for: Gap\n","disambiguation error for: Blue (disambiguation)\n","disambiguation error for: Outpost\n","disambiguation error for: Oat (disambiguation)\n","disambiguation error for: Sidetracked\n","disambiguation error for: Departure\n","total documents scraped till topic: 416, Adventure travel\n","  Scraping sub-topic: Beach destinations\n","300\n","total documents scraped till topic: 499, Beach destinations\n","  Scraping sub-topic: Travel deals\n","300\n","disambiguation error for: Economy (disambiguation)\n","total documents scraped till topic: 509, Travel deals\n","  Scraping sub-topic: Business trip\n","300\n","disambiguation error for: Trip\n","total documents scraped till topic: 520, Business trip\n","total documents scrapped:  520\n","total extra links:  151097\n","520\n"]}]},{"cell_type":"code","source":["food_results = scrape_wikipedia_subtopics(\"Food\", topics_list[\"Food\"], keywords[\"Food\"], 520)\n","total_documents[\"Food\"] = food_results\n","print(len(food_results))"],"metadata":{"id":"iQlTvo2byd6J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727475910832,"user_tz":240,"elapsed":1663732,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"outputId":"bb0f3284-409d-4111-b284-43842c79734f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping summaries for main topic: Food\n","  Scraping sub-topic: Organic Farming\n","300\n","disambiguation error for: Organic\n","disambiguation error for: Modern agriculture\n","total documents scraped till topic: 94, Organic Farming\n","  Scraping sub-topic: Crop yield statistics\n","300\n","disambiguation error for: Q (disambiguation)\n","total documents scraped till topic: 117, Crop yield statistics\n","  Scraping sub-topic: Global hunger\n","300\n","disambiguation error for: GHI\n","total documents scraped till topic: 170, Global hunger\n","  Scraping sub-topic: Food security\n","300\n","disambiguation error for: Security (disambiguation)\n","disambiguation error for: Food shortage\n","disambiguation error for: IPC\n","total documents scraped till topic: 294, Food security\n","  Scraping sub-topic: food quality\n","300\n","total documents scraped till topic: 344, food quality\n","  Scraping sub-topic: Protein food\n","300\n","total documents scraped till topic: 382, Protein food\n","  Scraping sub-topic: healthy drinks\n","300\n","disambiguation error for: Healthy food\n","total documents scraped till topic: 407, healthy drinks\n","  Scraping sub-topic: healthy food\n","300\n","disambiguation error for: Healthy food\n","disambiguation error for: The Book\n","total documents scraped till topic: 476, healthy food\n","  Scraping sub-topic: Fruits\n","300\n","disambiguation error for: Fruit (disambiguation)\n","total documents scraped till topic: 520, Fruits\n","total documents scrapped:  520\n","total extra links:  130309\n","520\n"]}]},{"cell_type":"code","source":["import json\n","\n","sw = common_stopwords = [\n","    'i', 'me', 'my', 'we', 'you', 'he', 'she', 'it', 'they',\n","    'what', 'which', 'who', 'this', 'that', 'these', 'those',\n","    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n","    'have', 'has', 'had', 'do', 'does', 'did', 'a', 'an', 'the',\n","    'and', 'but', 'if', 'or', 'because', 'as', 'with', 'for',\n","    'on', 'at', 'by', 'about', 'to', 'from', 'up', 'in', 'out'\n","]\n","# removing stopwords\n","def remove_stop_words(summary):\n","    words = summary.split()\n","    filtered_words = [word for word in words if word.lower() not in sw]\n","    return ' '.join(filtered_words)\n","\n","final_data = {}\n","\n","for topic in topics_list:\n","    documents = total_documents[topic]\n","    if len(documents) > 520:\n","        documents = documents[:520]\n","    preprocessed_documents = []\n","    for document in documents:\n","      preprocessed_summary = remove_stop_words(document['summary'])\n","      preprocessed_document = document.copy()\n","      preprocessed_document['summary'] = preprocessed_summary\n","      # convert revision_id to string\n","      preprocessed_document['revision_id'] = str(preprocessed_document['revision_id'])\n","      preprocessed_documents.append(preprocessed_document)\n","\n","    final_data[topic] = preprocessed_documents\n","\n","\n","for topic in final_data:\n","    print(f\"Total documents for {topic}: {len(final_data[topic])}\")\n","# save the data to json file\n","with open('preprocessed_documents.json', 'w') as json_file:\n","    json.dump(final_data, json_file, indent=4)\n"],"metadata":{"id":"Zk76vUEjnQ5w","executionInfo":{"status":"ok","timestamp":1727487629344,"user_tz":240,"elapsed":1092,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"915b41da-62c1-4cae-8cf1-a8f03016d910"},"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["Total documents for Health: 520\n","Total documents for Environment: 520\n","Total documents for Technology: 520\n","Total documents for Economy: 520\n","Total documents for Entertainment: 520\n","Total documents for Sports: 520\n","Total documents for Politics: 520\n","Total documents for Education: 520\n","Total documents for Travel: 520\n","Total documents for Food: 520\n"]}]},{"cell_type":"code","source":["import json\n","\n","# Data to be written to the JSON file\n","sub_data = {\n","    \"ip\": \"34.85.253.117\",\n","    \"port\": \"8983\",\n","    \"core\": \"IRF24P1\",\n","    \"ubit\": \"hemaboka\"\n","}\n","\n","# Specify the file name\n","file_name = 'hemaboka_p1.json'\n","\n","# Write data to the JSON file\n","with open(file_name, 'w') as json_file:\n","    json.dump(sub_data, json_file, indent=4)\n"],"metadata":{"id":"Af7l3PkGu6Ma","executionInfo":{"status":"ok","timestamp":1727495247102,"user_tz":240,"elapsed":169,"user":{"displayName":"Hema Bokam","userId":"11550102912662768106"}}},"execution_count":198,"outputs":[]}]}